{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq English-Polish Translation with PyTorch\n",
    "\n",
    "Character-level LSTM Encoder-Decoder for translation.\n",
    "\n",
    "**Architecture:**\n",
    "- Encoder: LSTM processes English sentence â†’ context vectors (h, c)\n",
    "- Decoder: LSTM generates Polish translation using context\n",
    "- Teacher forcing during training\n",
    "- Autoregressive generation during inference\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Setup Instructions for Kaggle\n",
    "\n",
    "1. **Upload this notebook** to Kaggle\n",
    "2. **Upload dataset**: Add `eng_to_pl.tsv` as input data\n",
    "   - Click \"Add Data\" â†’ \"Upload\" â†’ Select your `eng_to_pl.tsv` file\n",
    "   - Or create a Kaggle dataset first, then add it as input\n",
    "3. **Enable GPU**: Settings â†’ Accelerator â†’ GPU T4 x2 (or P100)\n",
    "4. **Run all cells**\n",
    "\n",
    "The notebook will automatically detect if it's running on Kaggle or locally and adjust the data path accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.389910Z",
     "iopub.status.busy": "2025-12-24T07:15:23.389621Z",
     "iopub.status.idle": "2025-12-24T07:15:23.395255Z",
     "shell.execute_reply": "2025-12-24T07:15:23.394547Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.389886Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available and device is set.\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "#     mps_device = torch.device(\"mps\")\n",
    "#     print(\"MPS is available and device is set.\")\n",
    "# else:\n",
    "#     print(\"MPS is not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Upload your `eng_to_pl.tsv` file to Kaggle's input folder, or adjust the path below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.397511Z",
     "iopub.status.busy": "2025-12-24T07:15:23.397266Z",
     "iopub.status.idle": "2025-12-24T07:15:23.577118Z",
     "shell.execute_reply": "2025-12-24T07:15:23.576447Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.397490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2723 translation pairs\n",
      "Vocabulary size: 58\n",
      "\n",
      "First 3 pairs:\n",
      "  'hurry up.' â†’ 'poÅ›piesz siÄ™!'\n",
      "  'so what?' â†’ 'no i co?'\n",
      "  'so what?' â†’ 'no i?'\n"
     ]
    }
   ],
   "source": [
    "class EnglishToPolishTranslationData:\n",
    "    def __init__(self, data_path=\"/Users/kamilstaszewski/Projects/math/data/eng_to_pl/eng_to_pl.tsv\"):\n",
    "        self.pairs = []\n",
    "        \n",
    "        if not Path(data_path).exists():\n",
    "            data_path = \"data/eng_to_pl/eng_to_pl.tsv\"  # Local path \n",
    "        \n",
    "        with open(data_path, encoding=\"utf-8\") as tsv_file:\n",
    "            reader = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "            for row in reader:\n",
    "                eng = row[1]\n",
    "                pol = row[3]\n",
    "                # Filter for shorter sentences (5-12 chars English, 5-15 chars Polish)\n",
    "                if len(eng) < 5 or len(eng) > 12:\n",
    "                    continue\n",
    "                if len(pol) < 5 or len(pol) > 15:\n",
    "                    continue\n",
    "                self.pairs.append((eng.lower(), pol.lower()))\n",
    "        \n",
    "        self.build_vocabulary()\n",
    "        print(f\"Loaded {len(self.pairs)} translation pairs\")\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "        all_chars = set().union(*(eng + pol for eng, pol in self.pairs))\n",
    "        self.chars = sorted(all_chars)\n",
    "\n",
    "        self.chars = [\"<PAD>\", \"<START>\", \"<END>\"] + self.chars\n",
    "        \n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "    \n",
    "    def char_to_onehot(self, char):\n",
    "        \"\"\"Convert character to one-hot vector (1, vocab_size)\"\"\"\n",
    "        char_idx = self.char_to_idx[char]\n",
    "        onehot = torch.zeros(1, self.vocab_size, device=device)\n",
    "        onehot[0, char_idx] = 1\n",
    "        return onehot\n",
    "    \n",
    "    def get_pairs(self):\n",
    "        return self.pairs\n",
    "\n",
    "data = EnglishToPolishTranslationData()\n",
    "print(f\"\\nFirst 3 pairs:\")\n",
    "for eng, pol in data.get_pairs()[:3]:\n",
    "    print(f\"  '{eng}' â†’ '{pol}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Encoder\n",
    "\n",
    "Processes English sentence character-by-character, outputs final hidden & cell states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.578419Z",
     "iopub.status.busy": "2025-12-24T07:15:23.578042Z",
     "iopub.status.idle": "2025-12-24T07:15:23.583523Z",
     "shell.execute_reply": "2025-12-24T07:15:23.582846Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.578388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "    \n",
    "    def encode(self, sentence, data_processor):\n",
    "        hidden = torch.zeros(1, self.hidden_size, device=device)\n",
    "        cell = torch.zeros(1, self.hidden_size, device=device)\n",
    "        encoder_states = []\n",
    "        \n",
    "        # Process each character\n",
    "        for char in sentence:\n",
    "            onehot = data_processor.char_to_onehot(char)\n",
    "            hidden, cell = self.lstm_cell(onehot, (hidden, cell))\n",
    "            encoder_states.append(hidden)\n",
    "        \n",
    "        return encoder_states, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.584698Z",
     "iopub.status.busy": "2025-12-24T07:15:23.584455Z",
     "iopub.status.idle": "2025-12-24T07:15:23.606037Z",
     "shell.execute_reply": "2025-12-24T07:15:23.605549Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.584678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_size, decoder_hidden_size, attention_size):\n",
    "        \"\"\"\n",
    "        Bahdanau attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_encoder = nn.Linear(encoder_hidden_size, attention_size, bias=False)\n",
    "\n",
    "        self.W_decoder = nn.Linear(decoder_hidden_size, attention_size, bias=False)\n",
    "\n",
    "        self.v = nn.Linear(attention_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_states):\n",
    "        encoder_outputs = torch.stack(encoder_states, dim=0)  # (seq_len, 1, hidden)\n",
    "        seq_len = encoder_outputs.size(0)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.squeeze(1)  # (seq_len, hidden)\n",
    "        decoder_hidden = decoder_hidden.squeeze(0)    # (hidden,)\n",
    "        \n",
    "        # Step 2: Project encoder states and decoder hidden state\n",
    "        # encoder_proj: (seq_len, attention_size)\n",
    "        encoder_proj = self.W_encoder(encoder_outputs)\n",
    "        \n",
    "        # decoder_proj: (attention_size,) â†’ expand to (seq_len, attention_size)\n",
    "        decoder_proj = self.W_decoder(decoder_hidden)\n",
    "        decoder_proj = decoder_proj.unsqueeze(0).expand(seq_len, -1)\n",
    "        \n",
    "        # Step 3: Compute alignment scores\n",
    "        # combined: (seq_len, attention_size)\n",
    "        combined = torch.tanh(encoder_proj + decoder_proj)\n",
    "        \n",
    "        # scores: (seq_len, 1)\n",
    "        scores = self.v(combined)\n",
    "        \n",
    "        # Step 4: Softmax to get attention weights\n",
    "        # attention_weights: (seq_len, 1)\n",
    "        attention_weights = F.softmax(scores, dim=0)\n",
    "        \n",
    "        # Step 5: Compute weighted sum of encoder states (context vector)\n",
    "        # context: (seq_len, hidden) * (seq_len, 1) â†’ sum â†’ (hidden,)\n",
    "        context_vector = (encoder_outputs * attention_weights).sum(dim=0)\n",
    "        \n",
    "        # Reshape back to (1, hidden) to match expected shape\n",
    "        context_vector = context_vector.unsqueeze(0)\n",
    "        attention_weights = attention_weights.squeeze(1).unsqueeze(0)  # (1, seq_len)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM Decoder\n",
    "\n",
    "Generates Polish translation using context from encoder.\n",
    "\n",
    "- **Training mode**: Teacher forcing (uses ground truth as input)\n",
    "- **Generation mode**: Autoregressive (uses its own predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.607825Z",
     "iopub.status.busy": "2025-12-24T07:15:23.607578Z",
     "iopub.status.idle": "2025-12-24T07:15:23.630825Z",
     "shell.execute_reply": "2025-12-24T07:15:23.630147Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.607807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.attention = Attention(\n",
    "            encoder_hidden_size=hidden_size,\n",
    "            decoder_hidden_size=hidden_size,\n",
    "            attention_size=128  # Can tune this\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def predict_char_probs(self, hidden_state):\n",
    "        logits = self.output_layer(hidden_state)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "    def decode_train(self, encoder_states, context_h, context_c, target_sentence, data):\n",
    "        hidden = context_h\n",
    "        cell = context_c\n",
    "        predictions = []\n",
    "        \n",
    "        current_char = \"<START>\"\n",
    "        \n",
    "        for target_char in target_sentence:\n",
    "            onehot = data.char_to_onehot(current_char)\n",
    "            hidden, cell = self.lstm_cell(onehot, (hidden, cell))\n",
    "            context_vector, attention_weights = self.attention(hidden, encoder_states)\n",
    "            combined = torch.cat([context_vector, hidden], dim = 1)\n",
    "            prediction = self.predict_char_probs(combined)\n",
    "            predictions.append(prediction)\n",
    "            \n",
    "            current_char = target_char  # Teacher forcing\n",
    "        \n",
    "        # Predict <END> token\n",
    "        onehot = data.char_to_onehot(current_char)\n",
    "        hidden, cell = self.lstm_cell(onehot, (hidden, cell))\n",
    "        context_vector, attention_weights = self.attention(hidden, encoder_states)\n",
    "        combined = torch.cat([context_vector, hidden], dim = 1)\n",
    "        prediction = self.predict_char_probs(combined)\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def decode_generate(self, encoder_states, context_h, context_c, data, max_length=30):\n",
    "        hidden = context_h\n",
    "        cell = context_c\n",
    "        generated = []\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        current_char = \"<START>\"\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            onehot = data.char_to_onehot(current_char)\n",
    "            hidden, cell = self.lstm_cell(onehot, (hidden, cell))\n",
    "            context_vector, attention_weights = self.attention(hidden, encoder_states)\n",
    "            all_attention_weights.append(attention_weights)\n",
    "            combined = torch.cat([context_vector, hidden], dim = 1)\n",
    "            log_probs = self.predict_char_probs(combined)\n",
    "            predicted_idx = log_probs.argmax().item()\n",
    "            next_char = data.idx_to_char[predicted_idx]\n",
    "            \n",
    "            if next_char == \"<END>\" or next_char == \"<PAD>\":\n",
    "                break\n",
    "            \n",
    "            generated.append(next_char)\n",
    "            current_char = next_char\n",
    "        \n",
    "        return \"\".join(generated), all_attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Function\n",
    "\n",
    "Negative Log-Likelihood Loss for sequence of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.631897Z",
     "iopub.status.busy": "2025-12-24T07:15:23.631646Z",
     "iopub.status.idle": "2025-12-24T07:15:23.653254Z",
     "shell.execute_reply": "2025-12-24T07:15:23.652723Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.631870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_sequence_loss(predictions, target_sequence, data_processor):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for prediction, target_char in zip(predictions, target_sequence):\n",
    "        target_onehot = data_processor.char_to_onehot(target_char)\n",
    "        target_idx = target_onehot.argmax().item()\n",
    "        target_tensor = torch.tensor([target_idx], device=device)\n",
    "        \n",
    "        loss = F.nll_loss(prediction, target_tensor)\n",
    "        total_loss += loss\n",
    "    \n",
    "    if len(predictions) > len(target_sequence):\n",
    "        last_prediction = predictions[-1]\n",
    "        end_onehot = data_processor.char_to_onehot(\"<END>\")\n",
    "        end_idx = end_onehot.argmax().item()\n",
    "        end_tensor = torch.tensor([end_idx], device=device)\n",
    "        \n",
    "        loss = F.nll_loss(last_prediction, end_tensor)\n",
    "        total_loss += loss\n",
    "    \n",
    "    avg_loss = total_loss / len(predictions)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Train encoder and decoder jointly with backpropagation through time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_attention(input_sentence, output_sentence, attention_weights):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        input_sentence: English input string\n",
    "        output_sentence: Polish output string\n",
    "        attention_weights: List of attention weight arrays, shape (1, input_len) each\n",
    "    \"\"\"\n",
    "    # Convert attention weights to 2D array: (output_len, input_len)\n",
    "    attention_matrix = np.vstack([w.squeeze() for w in attention_weights])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax.imshow(attention_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks(np.arange(len(input_sentence)))\n",
    "    ax.set_yticks(np.arange(len(output_sentence)))\n",
    "    ax.set_xticklabels(list(input_sentence), fontsize=12)\n",
    "    ax.set_yticklabels(list(output_sentence), fontsize=12)\n",
    "    \n",
    "    # Rotate x labels for better readability\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\")\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel('Input (English)', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Output (Polish)', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'Attention Weights: \"{input_sentence}\" â†’ \"{output_sentence}\"', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Attention Weight', rotation=270, labelpad=20, fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax.set_xticks(np.arange(len(input_sentence)) - 0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(output_sentence)) - 0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"gray\", linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.654466Z",
     "iopub.status.busy": "2025-12-24T07:15:23.654164Z",
     "iopub.status.idle": "2025-12-24T07:15:23.671359Z",
     "shell.execute_reply": "2025-12-24T07:15:23.670622Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.654437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_translation(encoder, decoder, data, test_sentences=None):\n",
    "    if test_sentences is None:\n",
    "        test_sentences = [\"i love you\", \"have fun\", \"why me?\"]\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for english in test_sentences:\n",
    "            encoder_states, context_h, context_c = encoder.encode(english, data)\n",
    "            polish, attention_weights = decoder.decode_generate(encoder_states, context_h, context_c, data, max_length=30)\n",
    "            print(f\"  {english} â†’ {polish}\")\n",
    "\n",
    "            # uncomments for heatmap\n",
    "            # if len(attention_weights) > 0:\n",
    "            #     visualize_attention(english, polish, attention_weights)\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "\n",
    "def train_network(epochs=200, lr=0.01, max_pairs=2723):\n",
    "    pairs = data.get_pairs()[:max_pairs]\n",
    "    print(f\"\\nTraining on {len(pairs)} pairs\\n\")\n",
    "    \n",
    "    encoder = Encoder(input_size=data.vocab_size, hidden_size=256).to(device)\n",
    "    decoder = Decoder(input_size=data.vocab_size, hidden_size=256, output_size=data.vocab_size).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for english, polish in pairs:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            encoder_states, context_h, context_c = encoder.encode(english, data)\n",
    "            \n",
    "            predictions = decoder.decode_train(encoder_states, context_h, context_c, polish, data)\n",
    "            \n",
    "            loss = compute_sequence_loss(predictions, polish, data)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "            list(encoder.parameters()) + list(decoder.parameters()),\n",
    "            max_norm=1.0\n",
    "        )\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            avg_loss = epoch_loss / len(pairs)\n",
    "            print(f\"Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "            test_translation(encoder, decoder, data)\n",
    "            print()\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return encoder, decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.672573Z",
     "iopub.status.busy": "2025-12-24T07:15:23.672300Z",
     "iopub.status.idle": "2025-12-24T07:19:36.239590Z",
     "shell.execute_reply": "2025-12-24T07:19:36.238656Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.672543Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on 1000 pairs\n",
      "\n",
      "Epoch 0/100, Loss: 2.7779\n",
      "  i love you â†’ za szesze.\n",
      "  have fun â†’ po szesze.\n",
      "  why me? â†’ po szesze.\n",
      "\n",
      "Epoch 10/100, Loss: 0.7828\n",
      "  i love you â†’ chodamy!\n",
      "  have fun â†’ on jest dobry!\n",
      "  why me? â†’ kaliy?\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m encoder, decoder = \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain_network\u001b[39m\u001b[34m(epochs, lr, max_pairs)\u001b[39m\n\u001b[32m     41\u001b[39m     loss = compute_sequence_loss(predictions, polish, data)\n\u001b[32m     42\u001b[39m     epoch_loss += loss.item()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     torch.nn.utils.clip_grad_norm_(\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mlist\u001b[39m(encoder.parameters()) + \u001b[38;5;28mlist\u001b[39m(decoder.parameters()),\n\u001b[32m     48\u001b[39m     max_norm=\u001b[32m1.0\u001b[39m\n\u001b[32m     49\u001b[39m )\n\u001b[32m     51\u001b[39m     optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/math/.venv/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/math/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/math/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "encoder, decoder = train_network(epochs=100, lr=0.01, max_pairs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on Custom Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T07:19:36.240126Z",
     "iopub.status.idle": "2025-12-24T07:19:36.240410Z",
     "shell.execute_reply": "2025-12-24T07:19:36.240299Z",
     "shell.execute_reply.started": "2025-12-24T07:19:36.240283Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom translations:\n",
      "  i love you â†’ kocham ciÄ™!\n",
      "  have fun â†’ obudÅº siÄ™.\n",
      "  why me? â†’ dlaczego namau.\n",
      "  hello â†’ pomocy!\n",
      "  good night â†’ dobranoc.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_sentences = [\n",
    "    \"i love you\",\n",
    "    \"have fun\",\n",
    "    \"why me?\",\n",
    "    \"hello\",\n",
    "    \"good night\"\n",
    "]\n",
    "\n",
    "print(\"\\nCustom translations:\")\n",
    "test_translation(encoder, decoder, data, custom_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T07:19:36.242017Z",
     "iopub.status.idle": "2025-12-24T07:19:36.242382Z",
     "shell.execute_reply": "2025-12-24T07:19:36.242201Z",
     "shell.execute_reply.started": "2025-12-24T07:19:36.242176Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved to 'seq2seq_eng_pol.pth'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.save({\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'decoder_state_dict': decoder.state_dict(),\n",
    "    'vocab_size': data.vocab_size,\n",
    "    'char_to_idx': data.char_to_idx,\n",
    "    'idx_to_char': data.idx_to_char\n",
    "}, 'seq2seq_eng_pol.pth')\n",
    "\n",
    "print(\"Models saved to 'seq2seq_eng_pol.pth'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Models (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T07:19:36.243889Z",
     "iopub.status.idle": "2025-12-24T07:19:36.244298Z",
     "shell.execute_reply": "2025-12-24T07:19:36.244150Z",
     "shell.execute_reply.started": "2025-12-24T07:19:36.244134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n",
      "  i love you â†’ kocham ciÄ™!\n",
      "  have fun â†’ obudÅº siÄ™.\n",
      "  why me? â†’ dlaczego namau.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint = torch.load('seq2seq_eng_pol.pth')\n",
    "\n",
    "\n",
    "loaded_encoder = Encoder(input_size=checkpoint['vocab_size'], hidden_size=256).to(device)\n",
    "loaded_decoder = Decoder(input_size=checkpoint['vocab_size'], hidden_size=256, output_size=checkpoint['vocab_size']).to(device)\n",
    "\n",
    "\n",
    "loaded_encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "loaded_decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "data.char_to_idx = checkpoint['char_to_idx']\n",
    "data.idx_to_char = checkpoint['idx_to_char']\n",
    "\n",
    "print(\"Models loaded successfully!\")\n",
    "test_translation(loaded_encoder, loaded_decoder, data)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9103669,
     "sourceId": 14266190,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
