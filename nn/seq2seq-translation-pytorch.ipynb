{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq English-Polish Translation with PyTorch\n",
    "\n",
    "Character-level LSTM Encoder-Decoder for translation.\n",
    "\n",
    "**Architecture:**\n",
    "- Encoder: LSTM processes English sentence â†’ context vectors (h, c)\n",
    "- Decoder: LSTM generates Polish translation using context\n",
    "- Teacher forcing during training\n",
    "- Autoregressive generation during inference\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Setup Instructions for Kaggle\n",
    "\n",
    "1. **Upload this notebook** to Kaggle\n",
    "2. **Upload dataset**: Add `eng_to_pl.tsv` as input data\n",
    "   - Click \"Add Data\" â†’ \"Upload\" â†’ Select your `eng_to_pl.tsv` file\n",
    "   - Or create a Kaggle dataset first, then add it as input\n",
    "3. **Enable GPU**: Settings â†’ Accelerator â†’ GPU T4 x2 (or P100)\n",
    "4. **Run all cells**\n",
    "\n",
    "The notebook will automatically detect if it's running on Kaggle or locally and adjust the data path accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.389910Z",
     "iopub.status.busy": "2025-12-24T07:15:23.389621Z",
     "iopub.status.idle": "2025-12-24T07:15:23.395255Z",
     "shell.execute_reply": "2025-12-24T07:15:23.394547Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.389886Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Upload your `eng_to_pl.tsv` file to Kaggle's input folder, or adjust the path below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.397511Z",
     "iopub.status.busy": "2025-12-24T07:15:23.397266Z",
     "iopub.status.idle": "2025-12-24T07:15:23.577118Z",
     "shell.execute_reply": "2025-12-24T07:15:23.576447Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.397490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2723 translation pairs\n",
      "Vocabulary size: 58\n",
      "\n",
      "First 3 pairs:\n",
      "  'hurry up.' â†’ 'poÅ›piesz siÄ™!'\n",
      "  'so what?' â†’ 'no i co?'\n",
      "  'so what?' â†’ 'no i?'\n"
     ]
    }
   ],
   "source": [
    "class EnglishToPolishTranslationData:\n",
    "    def __init__(self, data_path=\"/kaggle/input/eng-to-pl/eng_to_pl.tsv\"):\n",
    "        self.pairs = []\n",
    "        \n",
    "        if not Path(data_path).exists():\n",
    "            data_path = \"data/eng_to_pl/eng_to_pl.tsv\"  # Local path\n",
    "        \n",
    "        with open(data_path, encoding=\"utf-8\") as tsv_file:\n",
    "            reader = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "            for row in reader:\n",
    "                eng = row[1]\n",
    "                pol = row[3]\n",
    "                # Filter for shorter sentences (5-12 chars English, 5-15 chars Polish)\n",
    "                if len(eng) < 5 or len(eng) > 12:\n",
    "                    continue\n",
    "                if len(pol) < 5 or len(pol) > 15:\n",
    "                    continue\n",
    "                self.pairs.append((eng.lower(), pol.lower()))\n",
    "        \n",
    "        self.build_vocabulary()\n",
    "        print(f\"Loaded {len(self.pairs)} translation pairs\")\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "        all_chars = set().union(*(eng + pol for eng, pol in self.pairs))\n",
    "        self.chars = sorted(all_chars)\n",
    "\n",
    "        self.chars = [\"<PAD>\", \"<START>\", \"<END>\"] + self.chars\n",
    "        \n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "    \n",
    "    def char_to_onehot(self, char):\n",
    "        \"\"\"Convert character to one-hot vector (1, vocab_size)\"\"\"\n",
    "        char_idx = self.char_to_idx[char]\n",
    "        onehot = torch.zeros(1, self.vocab_size, device=device)\n",
    "        onehot[0, char_idx] = 1\n",
    "        return onehot\n",
    "    \n",
    "    def get_pairs(self):\n",
    "        return self.pairs\n",
    "\n",
    "data = EnglishToPolishTranslationData()\n",
    "print(f\"\\nFirst 3 pairs:\")\n",
    "for eng, pol in data.get_pairs()[:3]:\n",
    "    print(f\"  '{eng}' â†’ '{pol}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Encoder\n",
    "\n",
    "Processes English sentence character-by-character, outputs final hidden & cell states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.578419Z",
     "iopub.status.busy": "2025-12-24T07:15:23.578042Z",
     "iopub.status.idle": "2025-12-24T07:15:23.583523Z",
     "shell.execute_reply": "2025-12-24T07:15:23.582846Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.578388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "    \n",
    "    def encode(self, sentence, data_processor):\n",
    "        hidden = torch.zeros(1, self.hidden_size, device=device)\n",
    "        cell = torch.zeros(1, self.hidden_size, device=device)\n",
    "        encoder_states = []\n",
    "        \n",
    "        # Process each character\n",
    "        for char in sentence:\n",
    "            onehot = data_processor.char_to_onehot(char)\n",
    "            hidden, cell = self.lstm_cell(onehot, (hidden, cell))\n",
    "            encoder_states.append(hidden)\n",
    "        \n",
    "        return encoder_states, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.584698Z",
     "iopub.status.busy": "2025-12-24T07:15:23.584455Z",
     "iopub.status.idle": "2025-12-24T07:15:23.606037Z",
     "shell.execute_reply": "2025-12-24T07:15:23.605549Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.584678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_size, decoder_hidden_size, attention_size):\n",
    "        \"\"\"\n",
    "        Bahdanau attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_encoder = nn.Linear(encoder_hidden_size, attention_size, bias=False)\n",
    "\n",
    "        self.W_decoder = nn.Linear(decoder_hidden_size, attention_size, bias=False)\n",
    "\n",
    "        self.v = nn.Linear(attention_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_states):\n",
    "        encoder_outputs = torch.stack(encoder_states, dim=0)  # (seq_len, 1, hidden)\n",
    "        seq_len = encoder_outputs.size(0)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.squeeze(1)  # (seq_len, hidden)\n",
    "        decoder_hidden = decoder_hidden.squeeze(0)    # (hidden,)\n",
    "        \n",
    "        # Step 2: Project encoder states and decoder hidden state\n",
    "        # encoder_proj: (seq_len, attention_size)\n",
    "        encoder_proj = self.W_encoder(encoder_outputs)\n",
    "        \n",
    "        # decoder_proj: (attention_size,) â†’ expand to (seq_len, attention_size)\n",
    "        decoder_proj = self.W_decoder(decoder_hidden)\n",
    "        decoder_proj = decoder_proj.unsqueeze(0).expand(seq_len, -1)\n",
    "        \n",
    "        # Step 3: Compute alignment scores\n",
    "        # combined: (seq_len, attention_size)\n",
    "        combined = torch.tanh(encoder_proj + decoder_proj)\n",
    "        \n",
    "        # scores: (seq_len, 1)\n",
    "        scores = self.v(combined)\n",
    "        \n",
    "        # Step 4: Softmax to get attention weights\n",
    "        # attention_weights: (seq_len, 1)\n",
    "        attention_weights = F.softmax(scores, dim=0)\n",
    "        \n",
    "        # Step 5: Compute weighted sum of encoder states (context vector)\n",
    "        # context: (seq_len, hidden) * (seq_len, 1) â†’ sum â†’ (hidden,)\n",
    "        context_vector = (encoder_outputs * attention_weights).sum(dim=0)\n",
    "        \n",
    "        # Reshape back to (1, hidden) to match expected shape\n",
    "        context_vector = context_vector.unsqueeze(0)\n",
    "        attention_weights = attention_weights.squeeze(1).unsqueeze(0)  # (1, seq_len)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM Decoder\n",
    "\n",
    "Generates Polish translation using context from encoder.\n",
    "\n",
    "- **Training mode**: Teacher forcing (uses ground truth as input)\n",
    "- **Generation mode**: Autoregressive (uses its own predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.607825Z",
     "iopub.status.busy": "2025-12-24T07:15:23.607578Z",
     "iopub.status.idle": "2025-12-24T07:15:23.630825Z",
     "shell.execute_reply": "2025-12-24T07:15:23.630147Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.607807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.attention = Attention(\n",
    "            encoder_hidden_size=hidden_size,\n",
    "            decoder_hidden_size=hidden_size,\n",
    "            attention_size=128  # Can tune this\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def predict_char_probs(self, hidden_state):\n",
    "        logits = self.output_layer(hidden_state)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "    def decode_train(self, encoder_states, context_h, context_c, target_sentence, data):\n",
    "        hidden = context_h\n",
    "        cell = context_c\n",
    "        predictions = []\n",
    "        \n",
    "        current_char = \"<START>\"\n",
    "        \n",
    "        for target_char in target_sentence:\n",
    "            onehot = data.char_to_onehot(current_char)\n",
    "            hidden, cell = self.lstm_cell(onehot, (hidden, cell))\n",
    "            context_vector, attention_weights = self.attention(hidden, encoder_states)\n",
    "            combined = torch.cat([context_vector, hidden], dim = 1)\n",
    "            prediction = self.predict_char_probs(combined)\n",
    "            predictions.append(prediction)\n",
    "            \n",
    "            current_char = target_char  # Teacher forcing\n",
    "        \n",
    "        # Predict <END> token\n",
    "        onehot = data.char_to_onehot(current_char)\n",
    "        hidden, cell = self.lstm_cell(onehot, (hidden, cell))\n",
    "        context_vector, attention_weights = self.attention(hidden, encoder_states)\n",
    "        combined = torch.cat([context_vector, hidden], dim = 1)\n",
    "        prediction = self.predict_char_probs(combined)\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def decode_generate(self, encoder_states, context_h, context_c, data, max_length=30):\n",
    "        hidden = context_h\n",
    "        cell = context_c\n",
    "        generated = []\n",
    "        \n",
    "        current_char = \"<START>\"\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            onehot = data.char_to_onehot(current_char)\n",
    "            hidden, cell = self.lstm_cell(onehot, (hidden, cell))\n",
    "            context_vector, attention_weights = self.attention(hidden, encoder_states)\n",
    "            combined = torch.cat([context_vector, hidden], dim = 1)\n",
    "            log_probs = self.predict_char_probs(combined)\n",
    "            predicted_idx = log_probs.argmax().item()\n",
    "            next_char = data.idx_to_char[predicted_idx]\n",
    "            \n",
    "            if next_char == \"<END>\" or next_char == \"<PAD>\":\n",
    "                break\n",
    "            \n",
    "            generated.append(next_char)\n",
    "            current_char = next_char\n",
    "        \n",
    "        return \"\".join(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Function\n",
    "\n",
    "Negative Log-Likelihood Loss for sequence of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.631897Z",
     "iopub.status.busy": "2025-12-24T07:15:23.631646Z",
     "iopub.status.idle": "2025-12-24T07:15:23.653254Z",
     "shell.execute_reply": "2025-12-24T07:15:23.652723Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.631870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_sequence_loss(predictions, target_sequence, data_processor):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for prediction, target_char in zip(predictions, target_sequence):\n",
    "        target_onehot = data_processor.char_to_onehot(target_char)\n",
    "        target_idx = target_onehot.argmax().item()\n",
    "        target_tensor = torch.tensor([target_idx], device=device)\n",
    "        \n",
    "        loss = F.nll_loss(prediction, target_tensor)\n",
    "        total_loss += loss\n",
    "    \n",
    "    if len(predictions) > len(target_sequence):\n",
    "        last_prediction = predictions[-1]\n",
    "        end_onehot = data_processor.char_to_onehot(\"<END>\")\n",
    "        end_idx = end_onehot.argmax().item()\n",
    "        end_tensor = torch.tensor([end_idx], device=device)\n",
    "        \n",
    "        loss = F.nll_loss(last_prediction, end_tensor)\n",
    "        total_loss += loss\n",
    "    \n",
    "    avg_loss = total_loss / len(predictions)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Train encoder and decoder jointly with backpropagation through time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.654466Z",
     "iopub.status.busy": "2025-12-24T07:15:23.654164Z",
     "iopub.status.idle": "2025-12-24T07:15:23.671359Z",
     "shell.execute_reply": "2025-12-24T07:15:23.670622Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.654437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_translation(encoder, decoder, data, test_sentences=None):\n",
    "    if test_sentences is None:\n",
    "        test_sentences = [\"i love you\", \"have fun\", \"why me?\"]\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for english in test_sentences:\n",
    "            encoder_states, context_h, context_c = encoder.encode(english, data)\n",
    "            polish = decoder.decode_generate(encoder_states, context_h, context_c, data, max_length=30)\n",
    "            print(f\"  {english} â†’ {polish}\")\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "\n",
    "def train_network(epochs=200, lr=0.01, max_pairs=2723):\n",
    "    pairs = data.get_pairs()[:max_pairs]\n",
    "    print(f\"\\nTraining on {len(pairs)} pairs\\n\")\n",
    "    \n",
    "    encoder = Encoder(input_size=data.vocab_size, hidden_size=256).to(device)\n",
    "    decoder = Decoder(input_size=data.vocab_size, hidden_size=256, output_size=data.vocab_size).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for english, polish in pairs:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            encoder_states, context_h, context_c = encoder.encode(english, data)\n",
    "            \n",
    "            predictions = decoder.decode_train(encoder_states, context_h, context_c, polish, data)\n",
    "            \n",
    "            loss = compute_sequence_loss(predictions, polish, data)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "            list(encoder.parameters()) + list(decoder.parameters()),\n",
    "            max_norm=1.0\n",
    "        )\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            avg_loss = epoch_loss / len(pairs)\n",
    "            print(f\"Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "            test_translation(encoder, decoder, data)\n",
    "            print()\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return encoder, decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T07:15:23.672573Z",
     "iopub.status.busy": "2025-12-24T07:15:23.672300Z",
     "iopub.status.idle": "2025-12-24T07:19:36.239590Z",
     "shell.execute_reply": "2025-12-24T07:19:36.238656Z",
     "shell.execute_reply.started": "2025-12-24T07:15:23.672543Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on 1000 pairs\n",
      "\n",
      "Epoch 0/100, Loss: 2.7653\n",
      "  i love you â†’ po szesze.\n",
      "  have fun â†’ pa szesze.\n",
      "  why me? â†’ pa szesze.\n",
      "\n",
      "Epoch 10/100, Loss: 0.7680\n",
      "  i love you â†’ moÅ¼e pochoni.\n",
      "  have fun â†’ jak leci?\n",
      "  why me? â†’ kiedy?\n",
      "\n",
      "Epoch 20/100, Loss: 0.2621\n",
      "  i love you â†’ kocham ciÄ™!\n",
      "  have fun â†’ maw liÄ™ nie!\n",
      "  why me? â†’ dlaczego nia?\n",
      "\n",
      "Epoch 30/100, Loss: 0.1184\n",
      "  i love you â†’ kocham ciÄ™.\n",
      "  have fun â†’ na zdlionie!\n",
      "  why me? â†’ dlaczego ja?\n",
      "\n",
      "Epoch 40/100, Loss: 0.0809\n",
      "  i love you â†’ kocham ciÄ™!\n",
      "  have fun â†’ mar moÅ¼e!\n",
      "  why me? â†’ do kie?\n",
      "\n",
      "Epoch 50/100, Loss: 0.0684\n",
      "  i love you â†’ kocham ciÄ™!\n",
      "  have fun â†’ obudÅº siÄ™.\n",
      "  why me? â†’ dlaczego ja?\n",
      "\n",
      "Epoch 60/100, Loss: 0.0524\n",
      "  i love you â†’ kocham ciÄ™!\n",
      "  have fun â†’ marÄ™ miÄ™!\n",
      "  why me? â†’ dlaczego ja?\n",
      "\n",
      "Epoch 70/100, Loss: 0.0444\n",
      "  i love you â†’ kocham ciÄ™!\n",
      "  have fun â†’ mardzam!\n",
      "  why me? â†’ dlaczego ja?\n",
      "\n",
      "Epoch 80/100, Loss: 0.0408\n",
      "  i love you â†’ kocham ciÄ™!\n",
      "  have fun â†’ alery nie!\n",
      "  why me? â†’ dlaczego ja?\n",
      "\n",
      "Epoch 90/100, Loss: 0.0409\n",
      "  i love you â†’ kocham ciÄ™!\n",
      "  have fun â†’ obudÅº siÄ™.\n",
      "  why me? â†’ dlaczego ja?\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder = train_network(epochs=100, lr=0.01, max_pairs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on Custom Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T07:19:36.240126Z",
     "iopub.status.idle": "2025-12-24T07:19:36.240410Z",
     "shell.execute_reply": "2025-12-24T07:19:36.240299Z",
     "shell.execute_reply.started": "2025-12-24T07:19:36.240283Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom translations:\n",
      "  i love you â†’ kocham ciÄ™!\n",
      "  have fun â†’ obudÅº siÄ™.\n",
      "  why me? â†’ dlaczego namau.\n",
      "  hello â†’ pomocy!\n",
      "  good night â†’ dobranoc.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_sentences = [\n",
    "    \"i love you\",\n",
    "    \"have fun\",\n",
    "    \"why me?\",\n",
    "    \"hello\",\n",
    "    \"good night\"\n",
    "]\n",
    "\n",
    "print(\"\\nCustom translations:\")\n",
    "test_translation(encoder, decoder, data, custom_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T07:19:36.242017Z",
     "iopub.status.idle": "2025-12-24T07:19:36.242382Z",
     "shell.execute_reply": "2025-12-24T07:19:36.242201Z",
     "shell.execute_reply.started": "2025-12-24T07:19:36.242176Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved to 'seq2seq_eng_pol.pth'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.save({\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'decoder_state_dict': decoder.state_dict(),\n",
    "    'vocab_size': data.vocab_size,\n",
    "    'char_to_idx': data.char_to_idx,\n",
    "    'idx_to_char': data.idx_to_char\n",
    "}, 'seq2seq_eng_pol.pth')\n",
    "\n",
    "print(\"Models saved to 'seq2seq_eng_pol.pth'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Models (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-24T07:19:36.243889Z",
     "iopub.status.idle": "2025-12-24T07:19:36.244298Z",
     "shell.execute_reply": "2025-12-24T07:19:36.244150Z",
     "shell.execute_reply.started": "2025-12-24T07:19:36.244134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n",
      "  i love you â†’ kocham ciÄ™!\n",
      "  have fun â†’ obudÅº siÄ™.\n",
      "  why me? â†’ dlaczego namau.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint = torch.load('seq2seq_eng_pol.pth')\n",
    "\n",
    "\n",
    "loaded_encoder = Encoder(input_size=checkpoint['vocab_size'], hidden_size=256).to(device)\n",
    "loaded_decoder = Decoder(input_size=checkpoint['vocab_size'], hidden_size=256, output_size=checkpoint['vocab_size']).to(device)\n",
    "\n",
    "\n",
    "loaded_encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "loaded_decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "data.char_to_idx = checkpoint['char_to_idx']\n",
    "data.idx_to_char = checkpoint['idx_to_char']\n",
    "\n",
    "print(\"Models loaded successfully!\")\n",
    "test_translation(loaded_encoder, loaded_decoder, data)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9103669,
     "sourceId": 14266190,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
